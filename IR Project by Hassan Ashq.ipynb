{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semester Project : Information Retrieval System\n",
    " ### NUST\n",
    "- #### Submitted by : Hassan Ashiq BESE 23 C\n",
    "- ###### Link to my GitHub Repository <a href=\"https://github.com/hassanashiqasse/PCA\">Click Here</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from num2words import num2words\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading CISI.ALL File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "with open('CISI/CISI.ALL') as CISI_file:\n",
    "    lines = \"\"\n",
    "    for l in CISI_file.readlines():\n",
    "        lines += \"\\n\" + l.strip() if l.startswith(\".\") else \" \" + l.strip()\n",
    "    lines = lines.lstrip(\"\\n\").split(\"\\n\")\n",
    "    \n",
    "    \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placing each document in CISI File in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents = 1460.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A Contribution to the Theory of the Systems of Information Flows Kozachkov, L. S. Certain structural properties of information distributions are explored, as well as the gnosiological aspects of informational relations and the capabilities of an information retrieval system based on information distribution methods (\\\\\"MIR\\\\\").. '"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_set = {}\n",
    "doc_id = \"\"\n",
    "doc_text = \"\"\n",
    "for l in lines:\n",
    "    if l.startswith(\".I\"):\n",
    "        doc_id = l.split(\" \")[1].strip()\n",
    "    elif l.startswith(\".X\"):\n",
    "        doc_set[doc_id] = doc_text.lstrip(\" \")\n",
    "        doc_id = \"\"\n",
    "        doc_text = \"\"\n",
    "    else:\n",
    "        doc_text += l.strip()[3:] + \" \" # The first 3 characters of a line can be ignored.\n",
    "\n",
    "# Print something to see the dictionary structure, etc.\n",
    "print(f\"Number of documents = {len(doc_set)}\" + \".\\n\")\n",
    "\n",
    "doc_set['1125']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Two Kinds of Power An Essay on Bibliographic Control Wilson, P. The relationships between the organization and control of writings and the organization and control of knowledge and information will inevitably enter our story, for writings contain, along with much else, a great deal of mankind's stock of knowledge and information.  Bibliographical control is a form of power, and if knowledge itself is a form of power, as the familiar slogan claims, bibliographical control is in a certain sense power over power, power to obtain the knowledge recorded in written form.  As writings are not simply, and not in any simple way, storehouses of knowledge, we cannot satisfactorily discuss bibliographical control as simply control over the knowledge and information contained in writings. \""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_set[\"3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading CISI Query File and placing each query in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of queries = 112.\n",
      "\n",
      "Query # 2 :  How can actually pertinent data, as opposed to references or entire articles themselves, be retrieved automatically in response to information requests?\n"
     ]
    }
   ],
   "source": [
    "with open('CISI/CISI.QRY') as f:\n",
    "    lines = \"\"\n",
    "    for l in f.readlines():\n",
    "        lines += \"\\n\" + l.strip() if l.startswith(\".\") else \" \" + l.strip()\n",
    "    lines = lines.lstrip(\"\\n\").split(\"\\n\")\n",
    "    \n",
    "qry_set = {}\n",
    "qry_id = \"\"\n",
    "for l in lines:\n",
    "    if l.startswith(\".I\"):\n",
    "        qry_id = l.split(\" \")[1].strip()\n",
    "    elif l.startswith(\".W\"):\n",
    "        qry_set[qry_id] = l.strip()[3:]\n",
    "        qry_id = \"\"\n",
    "    \n",
    "# Print something to see the dictionary structure, etc.\n",
    "print(f\"Number of queries = {len(qry_set)}\" + \".\\n\")\n",
    "print(\"Query # 2 : \", qry_set[\"2\"]) # note that the dictionary indexes are strings, not numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What problems and concerns are there in making up descriptive titles? What difficulties are involved in automatically retrieving articles from approximate titles? What is the usual relevance of the content of articles to their titles?'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qry_set[\"1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Process of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lower_case(data):\n",
    "    return np.char.lower(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(data):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(data):\n",
    "    stemmer= PorterStemmer()\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_numbers(data):\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text = new_text + \" \" + w\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data = convert_lower_case(data)\n",
    "    data = remove_punctuation(data) #remove comma seperately\n",
    "    data = remove_apostrophe(data)\n",
    "    data = remove_stop_words(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = stemming(data)\n",
    "    data = remove_punctuation(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = stemming(data) #needed again as we need to stem the words\n",
    "    data = remove_punctuation(data) #needed again as num2word is giving few hypens and commas fourty-one\n",
    "    data = remove_stop_words(data) #needed again as num2word is giving stop words 101 - one hundred and one\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-b0bfcd4d8d7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc_set\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mdoc_token_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mprocessed_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc_token_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"done\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-67-08a2d032bb56>\u001b[0m in \u001b[0;36mpreprocess\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_numbers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstemming\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_punctuation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_numbers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstemming\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#needed again as we need to stem the words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-63-0dcb234ff6f1>\u001b[0m in \u001b[0;36mremove_punctuation\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msymbols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msymbols\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"  \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mreplace\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\defchararray.py\u001b[0m in \u001b[0;36mreplace\u001b[1;34m(a, old, new, count)\u001b[0m\n\u001b[0;32m   1169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1170\u001b[0m     \"\"\"\n\u001b[1;32m-> 1171\u001b[1;33m     return _to_string_or_unicode_array(\n\u001b[0m\u001b[0;32m   1172\u001b[0m         _vec_string(\n\u001b[0;32m   1173\u001b[0m             a, object_, 'replace', [old, new] + _clean_args(count)))\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\defchararray.py\u001b[0m in \u001b[0;36m_to_string_or_unicode_array\u001b[1;34m(result)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0man\u001b[0m \u001b[0mobject\u001b[0m \u001b[0marray\u001b[0m \u001b[0mmust\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0man\u001b[0m \u001b[0mintermediary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \"\"\"\n\u001b[1;32m---> 68\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_clean_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order, like)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "processed_set={}\n",
    "proc_token_id=\"\"\n",
    "proc_token_text=\"\"\n",
    "\n",
    "for i in doc_set:\n",
    "    doc_token_id=i\n",
    "    processed_set[doc_token_id]=preprocess(doc_set[str(i)])\n",
    "print(\"done\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Use Made of Technical Libraries Slater, M. This report is an analysis of 6300 acts of use in 104 technical libraries in the United Kingdom. Library use is only one aspect of the wider pattern of information use.  Information transfer in libraries is restricted to the use of documents.  It takes no account of documents used outside the library, still less of information transferred orally from person to person.  The library acts as a channel in only a proportion of the situations in which information is transferred. Taking technical information transfer as a whole, there is no doubt that this proportion is not the major one.  There are users of technical information - particularly in technology rather than science - who visit libraries rarely if at all, relying on desk collections of handbooks, current periodicals and personal contact with their colleagues and with people in other organizations.  Even regular library users also receive information in other ways. '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_set[\"2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' use made technic librari slater report analysi six thousand three hundr act use one hundr four technic librari unit kingdom librari use one aspect wider pattern inform use inform transfer librari restrict use document take account document use outsid librari still less inform transfer oral person person librari act channel proport situat inform transfer take technic inform transfer whole doubt proport major one user technic inform particularli technolog rather scienc visit librari rare reli desk collect handbook current period person contact colleagu peopl organ even regular librari user also receiv inform way'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_set[\"2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting prcessed text to tokens and placing in a dictionary where keys are the docs id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "tokens_set={}\n",
    "doc_token_id=\"\"\n",
    "doct_token_text=\"\"\n",
    "\n",
    "for i in processed_set:\n",
    "    doc_token_id=i\n",
    "    tokens_set[doc_token_id]=word_tokenize(processed_set[str(i)])\n",
    "print(\"done\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['use', 'made', 'technic', 'librari', 'slater', 'report', 'analysi',\n",
       "       'six', 'thousand', 'three', 'hundr', 'act', 'use', 'one', 'hundr',\n",
       "       'four', 'technic', 'librari', 'unit', 'kingdom', 'librari', 'use',\n",
       "       'one', 'aspect', 'wider', 'pattern', 'inform', 'use', 'inform',\n",
       "       'transfer', 'librari', 'restrict', 'use', 'document', 'take',\n",
       "       'account', 'document', 'use', 'outsid', 'librari', 'still', 'less',\n",
       "       'inform', 'transfer', 'oral', 'person', 'person', 'librari', 'act',\n",
       "       'channel', 'proport', 'situat', 'inform', 'transfer', 'take',\n",
       "       'technic', 'inform', 'transfer', 'whole', 'doubt', 'proport',\n",
       "       'major', 'one', 'user', 'technic', 'inform', 'particularli',\n",
       "       'technolog', 'rather', 'scienc', 'visit', 'librari', 'rare',\n",
       "       'reli', 'desk', 'collect', 'handbook', 'current', 'period',\n",
       "       'person', 'contact', 'colleagu', 'peopl', 'organ', 'even',\n",
       "       'regular', 'librari', 'user', 'also', 'receiv', 'inform', 'way'],\n",
       "      dtype='<U12')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tokens_set[\"2\"]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = {}\n",
    "\n",
    "for i in range(len(tokens_set)):\n",
    "    tokens = tokens_set[str(i+1)]\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}\n",
    "for i in DF:\n",
    "    DF[i] = len(DF[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eighteen': 15,\n",
       " 'edit': 44,\n",
       " 'dewey': 13,\n",
       " 'decim': 16,\n",
       " 'classif': 105,\n",
       " 'comaromi': 1,\n",
       " 'present': 318,\n",
       " 'studi': 362,\n",
       " 'histori': 52,\n",
       " 'first': 175,\n",
       " 'ddc': 5,\n",
       " 'publish': 122,\n",
       " 'one': 579,\n",
       " 'thousand': 350,\n",
       " 'eight': 127,\n",
       " 'hundr': 377,\n",
       " 'seventi': 134,\n",
       " 'six': 112,\n",
       " 'eighteenth': 1,\n",
       " 'nine': 299,\n",
       " 'futur': 95,\n",
       " 'continu': 68,\n",
       " 'appear': 86,\n",
       " 'need': 251,\n",
       " 'spite': 7,\n",
       " 'long': 48,\n",
       " 'healthi': 1,\n",
       " 'life': 39,\n",
       " 'howev': 111,\n",
       " 'full': 41,\n",
       " 'stori': 4,\n",
       " 'never': 15,\n",
       " 'told': 4,\n",
       " 'biographi': 3,\n",
       " 'briefli': 34,\n",
       " 'describ': 271,\n",
       " 'system': 515,\n",
       " 'attempt': 125,\n",
       " 'provid': 251,\n",
       " 'detail': 101,\n",
       " 'work': 252,\n",
       " 'spur': 3,\n",
       " 'growth': 67,\n",
       " 'librarianship': 49,\n",
       " 'countri': 46,\n",
       " 'abroad': 5,\n",
       " 'use': 659,\n",
       " 'made': 211,\n",
       " 'technic': 130,\n",
       " 'librari': 555,\n",
       " 'slater': 5,\n",
       " 'report': 185,\n",
       " 'analysi': 226,\n",
       " 'three': 212,\n",
       " 'act': 27,\n",
       " 'four': 143,\n",
       " 'unit': 94,\n",
       " 'kingdom': 10,\n",
       " 'aspect': 103,\n",
       " 'wider': 17,\n",
       " 'pattern': 85,\n",
       " 'inform': 660,\n",
       " 'transfer': 30,\n",
       " 'restrict': 25,\n",
       " 'document': 251,\n",
       " 'take': 66,\n",
       " 'account': 66,\n",
       " 'outsid': 19,\n",
       " 'still': 32,\n",
       " 'less': 66,\n",
       " 'oral': 4,\n",
       " 'person': 68,\n",
       " 'channel': 19,\n",
       " 'proport': 30,\n",
       " 'situat': 64,\n",
       " 'whole': 44,\n",
       " 'doubt': 12,\n",
       " 'major': 143,\n",
       " 'user': 235,\n",
       " 'particularli': 46,\n",
       " 'technolog': 110,\n",
       " 'rather': 77,\n",
       " 'scienc': 287,\n",
       " 'visit': 8,\n",
       " 'rare': 15,\n",
       " 'reli': 8,\n",
       " 'desk': 5,\n",
       " 'collect': 189,\n",
       " 'handbook': 10,\n",
       " 'current': 150,\n",
       " 'period': 112,\n",
       " 'contact': 10,\n",
       " 'colleagu': 12,\n",
       " 'peopl': 54,\n",
       " 'organ': 171,\n",
       " 'even': 89,\n",
       " 'regular': 25,\n",
       " 'also': 204,\n",
       " 'receiv': 49,\n",
       " 'way': 141,\n",
       " 'two': 304,\n",
       " 'kind': 66,\n",
       " 'power': 31,\n",
       " 'essay': 21,\n",
       " 'bibliograph': 149,\n",
       " 'control': 91,\n",
       " 'wilson': 4,\n",
       " 'relationship': 83,\n",
       " 'write': 34,\n",
       " 'knowledg': 101,\n",
       " 'inevit': 11,\n",
       " 'enter': 14,\n",
       " 'contain': 67,\n",
       " 'along': 29,\n",
       " 'much': 84,\n",
       " 'el': 6,\n",
       " 'great': 66,\n",
       " 'deal': 84,\n",
       " 'mankind': 4,\n",
       " 'stock': 12,\n",
       " 'form': 164,\n",
       " 'familiar': 15,\n",
       " 'slogan': 1,\n",
       " 'claim': 21,\n",
       " 'certain': 82,\n",
       " 'sen': 40,\n",
       " 'obtain': 94,\n",
       " 'record': 98,\n",
       " 'written': 48,\n",
       " 'simpli': 15,\n",
       " 'simpl': 50,\n",
       " 'storeh': 2,\n",
       " 'satisfactorili': 6,\n",
       " 'discuss': 262,\n",
       " 'univ': 179,\n",
       " 'final': 60,\n",
       " 'research': 362,\n",
       " 'project': 102,\n",
       " 'buckland': 6,\n",
       " 'establish': 106,\n",
       " 'new': 238,\n",
       " 'sixti': 197,\n",
       " 'provok': 3,\n",
       " 'highli': 40,\n",
       " 'stimul': 8,\n",
       " 'examin': 117,\n",
       " 'natur': 124,\n",
       " 'purpo': 142,\n",
       " 'manag': 96,\n",
       " 'academ': 88,\n",
       " 'attitud': 28,\n",
       " 'method': 266,\n",
       " 'question': 121,\n",
       " 'although': 58,\n",
       " 'chang': 117,\n",
       " 'basic': 130,\n",
       " 'difficulti': 35,\n",
       " 'remain': 38,\n",
       " 'lack': 28,\n",
       " 'object': 104,\n",
       " 'best': 46,\n",
       " 'servic': 267,\n",
       " 'ugc': 2,\n",
       " 'committ': 30,\n",
       " 'parri': 2,\n",
       " 'repot': 2,\n",
       " 'seven': 103,\n",
       " 'gener': 330,\n",
       " 'endor': 2,\n",
       " 'stress': 19,\n",
       " 'provi': 18,\n",
       " 'game': 4,\n",
       " 'brophi': 2,\n",
       " 'profess': 100,\n",
       " 'educ': 92,\n",
       " 'becom': 91,\n",
       " 'widespread': 12,\n",
       " 'last': 57,\n",
       " 'decad': 36,\n",
       " 'number': 206,\n",
       " 'field': 193,\n",
       " 'mani': 195,\n",
       " 'year': 225,\n",
       " 'origin': 82,\n",
       " 'trace': 14,\n",
       " 'war': 13,\n",
       " 'militari': 8,\n",
       " 'train': 39,\n",
       " 'real': 27,\n",
       " 'thing': 16,\n",
       " 'either': 62,\n",
       " 'unavail': 4,\n",
       " 'danger': 8,\n",
       " 'recent': 93,\n",
       " 'time': 220,\n",
       " 'sophist': 14,\n",
       " 'larg': 188,\n",
       " 'electron': 28,\n",
       " 'comput': 276,\n",
       " 'handl': 55,\n",
       " 'complex': 54,\n",
       " 'calcul': 21,\n",
       " 'involv': 80,\n",
       " 'sinc': 99,\n",
       " 'fifti': 109,\n",
       " 'well': 150,\n",
       " 'develop': 378,\n",
       " 'introduc': 37,\n",
       " 'techniqu': 162,\n",
       " 'spread': 11,\n",
       " 'rapidli': 32,\n",
       " 'wide': 60,\n",
       " 'varieti': 53,\n",
       " 'disciplin': 60,\n",
       " 'today': 34,\n",
       " 'level': 72,\n",
       " 'primari': 43,\n",
       " 'school': 70,\n",
       " 'class': 51,\n",
       " 'cour': 61,\n",
       " 'experienc': 14,\n",
       " 'men': 17,\n",
       " 'women': 5,\n",
       " 'main': 59,\n",
       " 'cau': 27,\n",
       " 'explo': 13,\n",
       " 'rapid': 45,\n",
       " 'simul': 18,\n",
       " 'mathemat': 65,\n",
       " 'model': 103,\n",
       " 'possibl': 176,\n",
       " 'advanc': 69,\n",
       " 'abstract': 118,\n",
       " 'concept': 155,\n",
       " 'borko': 10,\n",
       " 'graduat': 29,\n",
       " 'includ': 168,\n",
       " 'materi': 119,\n",
       " 'characteristc': 1,\n",
       " 'type': 141,\n",
       " 'histor': 29,\n",
       " 'public': 186,\n",
       " 'industri': 86,\n",
       " 'especi': 34,\n",
       " 'state': 142,\n",
       " 'standard': 88,\n",
       " 'prepar': 75,\n",
       " 'evalu': 174,\n",
       " 'product': 107,\n",
       " 'topic': 41,\n",
       " 'call': 69,\n",
       " 'text': 88,\n",
       " 'section': 28,\n",
       " 'instruct': 23,\n",
       " 'variou': 136,\n",
       " 'supplement': 15,\n",
       " 'exampl': 103,\n",
       " 'exerci': 11,\n",
       " 'appendix': 7,\n",
       " 'brief': 21,\n",
       " 'index': 254,\n",
       " 'autom': 53,\n",
       " 'treat': 31,\n",
       " 'exten': 36,\n",
       " 'believ': 32,\n",
       " 'deserv': 6,\n",
       " 'greater': 34,\n",
       " 'emphasi': 46,\n",
       " 'past': 68,\n",
       " 'increasingli': 30,\n",
       " 'import': 153,\n",
       " 'effort': 89,\n",
       " 'expend': 9,\n",
       " 'extract': 17,\n",
       " 'student': 71,\n",
       " 'librarian': 162,\n",
       " 'abstractor': 3,\n",
       " 'benefit': 30,\n",
       " 'know': 26,\n",
       " 'understand': 58,\n",
       " 'program': 175,\n",
       " 'analyz': 94,\n",
       " 'select': 178,\n",
       " 'key': 37,\n",
       " 'sentenc': 14,\n",
       " 'segment': 12,\n",
       " 'opportun': 17,\n",
       " 'avail': 123,\n",
       " 'part': 126,\n",
       " 'volunt': 2,\n",
       " 'worker': 23,\n",
       " 'find': 115,\n",
       " 'activ': 124,\n",
       " 'pleasant': 1,\n",
       " 'reward': 16,\n",
       " 'contribut': 60,\n",
       " 'effect': 187,\n",
       " 'store': 31,\n",
       " 'chapter': 51,\n",
       " 'devot': 33,\n",
       " 'career': 13,\n",
       " 'build': 38,\n",
       " 'guid': 45,\n",
       " 'architectur': 3,\n",
       " 'issu': 50,\n",
       " 'solut': 55,\n",
       " 'ellsworth': 4,\n",
       " 'book': 295,\n",
       " 'repr': 94,\n",
       " 'success': 55,\n",
       " 'problem': 313,\n",
       " 'architect': 3,\n",
       " 'face': 34,\n",
       " 'plan': 100,\n",
       " 'colleg': 53,\n",
       " 'remodel': 1,\n",
       " 'enlarg': 4,\n",
       " 'exist': 98,\n",
       " 'structur': 173,\n",
       " 'make': 167,\n",
       " 'case': 80,\n",
       " 'done': 27,\n",
       " 'mason': 4,\n",
       " 'brown': 8,\n",
       " 'yale': 12,\n",
       " 'unsuccess': 1,\n",
       " 'except': 33,\n",
       " 'show': 97,\n",
       " 'avoid': 17,\n",
       " 'mistak': 1,\n",
       " 'identifi': 89,\n",
       " 'honor': 3,\n",
       " 'guy': 1,\n",
       " 'lyle': 4,\n",
       " 'farber': 1,\n",
       " 'staff': 50,\n",
       " 'member': 53,\n",
       " 'individu': 91,\n",
       " 'apprenticeship': 1,\n",
       " 'administr': 60,\n",
       " 'perhap': 32,\n",
       " 'signif': 95,\n",
       " 'acquir': 23,\n",
       " 'engend': 1,\n",
       " 'insist': 3,\n",
       " 'must': 90,\n",
       " 'interest': 134,\n",
       " 'content': 74,\n",
       " 'dealt': 13,\n",
       " 'love': 1,\n",
       " 'literatur': 221,\n",
       " 'respect': 58,\n",
       " 'scholarship': 8,\n",
       " 'admir': 4,\n",
       " 'good': 47,\n",
       " 'read': 45,\n",
       " 'manifest': 9,\n",
       " 'notabl': 7,\n",
       " 'admonit': 1,\n",
       " 'though': 31,\n",
       " 'primarili': 44,\n",
       " 'constantli': 7,\n",
       " 'keep': 27,\n",
       " 'mind': 22,\n",
       " 'oblig': 7,\n",
       " 'contemporari': 17,\n",
       " 'poetri': 2,\n",
       " 'fiction': 1,\n",
       " 'bell': 4,\n",
       " 'letter': 15,\n",
       " 'felt': 16,\n",
       " 'respon': 63,\n",
       " 'cross': 17,\n",
       " 'disciplinari': 7,\n",
       " 'line': 119,\n",
       " 'fell': 2,\n",
       " 'faculti': 23,\n",
       " 'mostli': 6,\n",
       " 'concern': 159,\n",
       " 'apt': 4,\n",
       " 'overlook': 7,\n",
       " 'portion': 11,\n",
       " 'substitut': 13,\n",
       " 'thorough': 9,\n",
       " 'acquaint': 5,\n",
       " 'critic': 65,\n",
       " 'review': 111,\n",
       " 'counsel': 2,\n",
       " 'presid': 5,\n",
       " 'professor': 9,\n",
       " 'thrust': 2,\n",
       " 'world': 72,\n",
       " 'impress': 6,\n",
       " 'upon': 67,\n",
       " 'us': 43,\n",
       " 'access': 80,\n",
       " 'hyman': 1,\n",
       " 'assum': 40,\n",
       " 'addit': 79,\n",
       " 'held': 20,\n",
       " 'promi': 16,\n",
       " 'analyt': 19,\n",
       " 'consid': 223,\n",
       " 'approach': 138,\n",
       " 'survey': 121,\n",
       " 'compar': 126,\n",
       " 'tradit': 55,\n",
       " 'idea': 59,\n",
       " 'direct': 66,\n",
       " 'princip': 30,\n",
       " 'data': 304,\n",
       " 'gather': 32,\n",
       " 'instrument': 20,\n",
       " 'documentari': 7,\n",
       " 'opinion': 26,\n",
       " 'questionnair': 35,\n",
       " 'follow': 102,\n",
       " 'nineti': 50,\n",
       " 'shelf': 12,\n",
       " 'brow': 6,\n",
       " 'left': 14,\n",
       " 'unresolv': 2,\n",
       " 'evid': 39,\n",
       " 'resist': 5,\n",
       " 'exhaust': 16,\n",
       " 'confirm': 7,\n",
       " 'open': 14,\n",
       " 'rel': 89,\n",
       " 'locat': 32,\n",
       " 'meant': 5,\n",
       " 'arou': 3,\n",
       " 'intellectu': 27,\n",
       " 'social': 128,\n",
       " 'polit': 15,\n",
       " 'averag': 37,\n",
       " 'citizen': 7,\n",
       " 'affect': 48,\n",
       " 'democrat': 3,\n",
       " 'self': 24,\n",
       " 'realiz': 14,\n",
       " 'definit': 54,\n",
       " 'vari': 31,\n",
       " 'greatli': 22,\n",
       " 'indulg': 2,\n",
       " 'untutor': 1,\n",
       " 'benefici': 5,\n",
       " 'reader': 95,\n",
       " 'valuabl': 23,\n",
       " 'guidanc': 8,\n",
       " 'scholar': 16,\n",
       " 'resourc': 70,\n",
       " 'palmour': 3,\n",
       " 'recommend': 43,\n",
       " 'nation': 131,\n",
       " 'improv': 115,\n",
       " 'forti': 80,\n",
       " 'percent': 26,\n",
       " 'interlibrari': 10,\n",
       " 'loan': 15,\n",
       " 'bulk': 3,\n",
       " 'satisfi': 31,\n",
       " 'photocopi': 10,\n",
       " 'rang': 42,\n",
       " 'augment': 8,\n",
       " 'request': 70,\n",
       " 'focu': 36,\n",
       " 'physic': 75,\n",
       " 'base': 286,\n",
       " 'commun': 194,\n",
       " 'design': 194,\n",
       " 'featur': 43,\n",
       " 'without': 68,\n",
       " 'initi': 43,\n",
       " 'confin': 4,\n",
       " 'depend': 46,\n",
       " 'deliveri': 10,\n",
       " 'journal': 143,\n",
       " 'articl': 131,\n",
       " 'center': 86,\n",
       " 'comprehen': 43,\n",
       " 'subject': 235,\n",
       " 'coverag': 32,\n",
       " 'exclud': 11,\n",
       " 'medicin': 29,\n",
       " 'worthwhil': 4,\n",
       " 'irrespect': 3,\n",
       " 'languag': 123,\n",
       " 'acquisit': 40,\n",
       " 'ford': 5,\n",
       " 'scope': 36,\n",
       " 'outlin': 52,\n",
       " 'introduct': 35,\n",
       " 'acknowledg': 7,\n",
       " 'polici': 46,\n",
       " 'serial': 48,\n",
       " 'kindr': 1,\n",
       " 'relat': 226,\n",
       " 'thoroughli': 7,\n",
       " 'paper': 268,\n",
       " 'cite': 46,\n",
       " 'refer': 165,\n",
       " 'note': 37,\n",
       " 'central': 44,\n",
       " 'order': 115,\n",
       " 'routin': 9,\n",
       " 'manual': 45,\n",
       " 'practic': 160,\n",
       " 'treatment': 21,\n",
       " 'particular': 113,\n",
       " 'depth': 20,\n",
       " 'modest': 10,\n",
       " 'enorm': 11,\n",
       " '3rd': 2,\n",
       " 'clark': 2,\n",
       " 'ligu': 1,\n",
       " 'de': 11,\n",
       " 'bibliothequ': 1,\n",
       " 'europeenn': 1,\n",
       " 'recherch': 1,\n",
       " 'liber': 4,\n",
       " 'set': 115,\n",
       " 'intern': 87,\n",
       " 'non': 30,\n",
       " 'govern': 62,\n",
       " 'aim': 54,\n",
       " 'close': 40,\n",
       " 'collabor': 14,\n",
       " 'western': 9,\n",
       " 'europ': 5,\n",
       " 'help': 73,\n",
       " 'qualiti': 42,\n",
       " 'second': 60,\n",
       " 'meet': 59,\n",
       " 'assembl': 10,\n",
       " 'luxembourg': 1,\n",
       " 'decid': 25,\n",
       " 'hold': 38,\n",
       " 'seminar': 9,\n",
       " 'third': 37,\n",
       " 'charg': 25,\n",
       " 'would': 123,\n",
       " 'european': 7,\n",
       " 'lend': 15,\n",
       " 'feasibl': 35,\n",
       " 'centr': 10,\n",
       " 'prefer': 29,\n",
       " 'machin': 112,\n",
       " 'readabl': 39,\n",
       " 'whatev': 13,\n",
       " 'mean': 153,\n",
       " 'propo': 95,\n",
       " 'area': 126,\n",
       " 'council': 17,\n",
       " 'grant': 14,\n",
       " 'toward': 74,\n",
       " 'cost': 150,\n",
       " 'sussex': 1,\n",
       " 'seventeen': 10,\n",
       " 'nineteen': 11,\n",
       " 'septemb': 11,\n",
       " 'ad695049': 1,\n",
       " 'wooster': 1,\n",
       " 'ever': 21,\n",
       " 'pretend': 2,\n",
       " 'expert': 19,\n",
       " 'microfich': 6,\n",
       " 'nevertheless': 11,\n",
       " 'invit': 7,\n",
       " 'address': 24,\n",
       " 'annual': 33,\n",
       " 'northeastern': 1,\n",
       " 'confer': 30,\n",
       " 'waltham': 1,\n",
       " 'massachusett': 4,\n",
       " 'april': 11,\n",
       " 'temer': 1,\n",
       " 'like': 79,\n",
       " 'fich': 2,\n",
       " 'uniform': 9,\n",
       " 'feder': 21,\n",
       " 'cuddli': 1,\n",
       " 'revi': 22,\n",
       " 'ad': 33,\n",
       " 'thesi': 6,\n",
       " 'rest': 5,\n",
       " 'frequent': 38,\n",
       " 'neglect': 8,\n",
       " 'principl': 74,\n",
       " 'goe': 10,\n",
       " 'beyond': 17,\n",
       " 'commonli': 12,\n",
       " 'accept': 56,\n",
       " 'function': 108,\n",
       " 'circul': 38,\n",
       " 'storag': 84,\n",
       " 'mere': 25,\n",
       " 'hou': 16,\n",
       " 'extend': 30,\n",
       " 'teach': 27,\n",
       " 'process': 216,\n",
       " 'afford': 9,\n",
       " 'encourag': 16,\n",
       " 'better': 52,\n",
       " 'essenti': 46,\n",
       " 'tool': 59,\n",
       " 'fulli': 29,\n",
       " 'effici': 81,\n",
       " 'retain': 4,\n",
       " 'substanti': 26,\n",
       " 'independ': 32,\n",
       " 'found': 94,\n",
       " 'footnot': 5,\n",
       " 'bibliographi': 52,\n",
       " 'brought': 22,\n",
       " 'date': 40,\n",
       " 'flow': 42,\n",
       " 'laboratori': 25,\n",
       " 'allen': 6,\n",
       " 'thoma': 9,\n",
       " 'cohen': 1,\n",
       " 'stephen': 9,\n",
       " 'modifi': 15,\n",
       " 'sociometr': 4,\n",
       " 'network': 63,\n",
       " 'result': 291,\n",
       " 'interact': 61,\n",
       " 'star': 1,\n",
       " 'adopt': 37,\n",
       " 'congress': 32,\n",
       " 'matthi': 1,\n",
       " 'procedur': 105,\n",
       " 'may': 187,\n",
       " 'serv': 77,\n",
       " 'exact': 8,\n",
       " 'seri': 51,\n",
       " 'suggest': 143,\n",
       " 'step': 45,\n",
       " 'proven': 6,\n",
       " 'actual': 62,\n",
       " 'necessari': 72,\n",
       " 'criteria': 47,\n",
       " 'reclassif': 2,\n",
       " 'attent': 65,\n",
       " 'given': 157,\n",
       " 'lc': 13,\n",
       " 'law': 26,\n",
       " 'pz3': 1,\n",
       " 'pz4': 1,\n",
       " 'tabl': 34,\n",
       " 'viiia': 1,\n",
       " 'ixa': 1,\n",
       " 'throughout': 28,\n",
       " 'entir': 32,\n",
       " 'explain': 36,\n",
       " 'applic': 147,\n",
       " 'illustr': 49,\n",
       " 'mechan': 71,\n",
       " 'catalogu': 38,\n",
       " 'card': 51,\n",
       " 'copi': 26,\n",
       " 'significantli': 16,\n",
       " 'xerox': 3,\n",
       " 'oper': 203,\n",
       " 'annot': 7,\n",
       " 'judg': 28,\n",
       " 'reclassifi': 1,\n",
       " 'wish': 17,\n",
       " 'delv': 1,\n",
       " 'deepli': 3,\n",
       " 'tortuou': 1,\n",
       " 'frustrat': 4,\n",
       " 'catalog': 109,\n",
       " 'enclo': 1,\n",
       " 'parenth': 1,\n",
       " 'sourc': 102,\n",
       " 'support': 72,\n",
       " 'argument': 15,\n",
       " 'adventur': 3,\n",
       " 'voigt': 1,\n",
       " 'scholarli': 12,\n",
       " 'fill': 15,\n",
       " 'progress': 66,\n",
       " 'obviou': 14,\n",
       " 'front': 5,\n",
       " 'caught': 1,\n",
       " 'enthusiast': 2,\n",
       " 'visual': 7,\n",
       " 'potenti': 64,\n",
       " 'certainli': 16,\n",
       " 'everi': 45,\n",
       " 'volum': 80,\n",
       " 'group': 133,\n",
       " 'demonstr': 37,\n",
       " 'implic': 46,\n",
       " 'inventori': 15,\n",
       " 'expect': 46,\n",
       " 'dedic': 3,\n",
       " 'realiti': 6,\n",
       " 'assess': 47,\n",
       " 'go': 27,\n",
       " 'fast': 8,\n",
       " 'hope': 52,\n",
       " 'get': 19,\n",
       " 'statu': 31,\n",
       " 'produc': 98,\n",
       " 'lie': 10,\n",
       " 'ahead': 3,\n",
       " 'age': 27,\n",
       " 'marc': 26,\n",
       " 'busi': 30,\n",
       " 'easier': 8,\n",
       " 'immedi': 26,\n",
       " 'file': 105,\n",
       " 'manipul': 19,\n",
       " 'defend': 2,\n",
       " 'adequ': 46,\n",
       " 'fact': 74,\n",
       " 'determin': 119,\n",
       " 'difficult': 31,\n",
       " 'widen': 4,\n",
       " 'dimen': 12,\n",
       " 'reflect': 48,\n",
       " 'name': 53,\n",
       " 'media': 25,\n",
       " 'togeth': 48,\n",
       " 'children': 4,\n",
       " 'clear': 27,\n",
       " 'rule': 57,\n",
       " 'bibliotherapi': 1,\n",
       " 'slow': 5,\n",
       " 'theori': 140,\n",
       " 'manageri': 8,\n",
       " 'come': 36,\n",
       " 'provoc': 2,\n",
       " 'controversi': 13,\n",
       " 'degr': 43,\n",
       " 'assist': 40,\n",
       " 'modern': 39,\n",
       " 'dissemin': 69,\n",
       " 'mauerhoff': 2,\n",
       " 'duplic': 20,\n",
       " 'previou': 63,\n",
       " 'complement': 9,\n",
       " 'earlier': 35,\n",
       " 'gap': 13,\n",
       " 'prior': 18,\n",
       " 'bold': 2,\n",
       " 'sdi': 38,\n",
       " 'descript': 102,\n",
       " 'luhn': 6,\n",
       " '1961b': 1,\n",
       " 'post': 19,\n",
       " 'boom': 2,\n",
       " 'began': 18,\n",
       " 'lose': 3,\n",
       " 'ground': 9,\n",
       " 'popular': 9,\n",
       " 'therefor': 66,\n",
       " 'interpret': 41,\n",
       " 'implement': 42,\n",
       " 'evolut': 13,\n",
       " 'light': 18,\n",
       " 'perform': 137,\n",
       " 'compani': 11,\n",
       " 'agenc': 22,\n",
       " 'societi': 48,\n",
       " 'fourteen': 10,\n",
       " 'herdan': 1,\n",
       " 'tri': 36,\n",
       " 'give': 66,\n",
       " 'statist': 102,\n",
       " 'properti': 42,\n",
       " 'common': 44,\n",
       " 'thread': 6,\n",
       " 'multifari': 1,\n",
       " 'embodi': 3,\n",
       " 'scatter': 21,\n",
       " 'linguist': 27,\n",
       " 'philosoph': 13,\n",
       " 'mathematician': 7,\n",
       " 'engin': 68,\n",
       " 'idiom': 1,\n",
       " 'belong': 11,\n",
       " 'quantit': 41,\n",
       " 'peculiar': 5,\n",
       " 'convent': 37,\n",
       " 'put': 34,\n",
       " 'differ': 197,\n",
       " 'latter': 32,\n",
       " 'compri': 9,\n",
       " 'paramet': 27,\n",
       " 'appli': 104,\n",
       " 'econom': 73,\n",
       " 'demographi': 1,\n",
       " 'former': 17,\n",
       " 'characteri': 2,\n",
       " 'adumbr': 1,\n",
       " 'author': 155,\n",
       " 'choic': 32,\n",
       " 'chanc': 7,\n",
       " 'taken': 47,\n",
       " 'shape': 10,\n",
       " 'shall': 18,\n",
       " 'foundat': 19,\n",
       " 'laid': 5,\n",
       " 'truli': 3,\n",
       " 'sensibl': 4,\n",
       " 'langu': 1,\n",
       " 'parol': 1,\n",
       " 'dichotomi': 1,\n",
       " 'sampl': 57,\n",
       " 'jewett': 1,\n",
       " 'charl': 7,\n",
       " 'coffin': 1,\n",
       " 'american': 73,\n",
       " 'harri': 6,\n",
       " 'mark': 16,\n",
       " 'begin': 37,\n",
       " 'associ': 105,\n",
       " 'scene': 7,\n",
       " 'dynam': 19,\n",
       " 'figur': 34,\n",
       " 'melvil': 1,\n",
       " 'ammi': 1,\n",
       " 'cutter': 5,\n",
       " 'extrem': 9,\n",
       " 'era': 6,\n",
       " 'quarter': 13,\n",
       " 'centuri': 29,\n",
       " 'prece': 6,\n",
       " 'philadelphia': 4,\n",
       " 'charact': 57,\n",
       " 'influenc': 52,\n",
       " 'america': 7,\n",
       " 'ignor': 12,\n",
       " 'nineteenth': 3,\n",
       " 'risk': 3,\n",
       " 'misinterpret': 1,\n",
       " 'pivot': 2,\n",
       " 'reapprai': 3,\n",
       " 'seem': 73,\n",
       " 'appropri': 56,\n",
       " 'aggress': 2,\n",
       " 'lorenz': 2,\n",
       " 'vertebr': 1,\n",
       " 'point': 113,\n",
       " 'total': 71,\n",
       " 'predat': 1,\n",
       " 'biolog': 28,\n",
       " 'necess': 13,\n",
       " 'defenc': 1,\n",
       " 'territori': 2,\n",
       " 'corner': 2,\n",
       " 'anim': 4,\n",
       " 'mix': 2,\n",
       " 'innat': 2,\n",
       " 'drive': 4,\n",
       " 'thu': 61,\n",
       " 'lead': 45,\n",
       " 'reduct': 17,\n",
       " 'intraspecif': 1,\n",
       " 'damag': 1,\n",
       " 'fiercer': 1,\n",
       " 'wolv': 1,\n",
       " 'escap': 1,\n",
       " 'pack': 2,\n",
       " 'virtual': 10,\n",
       " 'imposs': 4,\n",
       " 'co': 38,\n",
       " 'fight': 3,\n",
       " 'surviv': 10,\n",
       " 'wherea': 11,\n",
       " 'proverbi': 1,\n",
       " 'peac': 3,\n",
       " 'dove': 1,\n",
       " 'prevent': 14,\n",
       " 'violent': 1,\n",
       " 'often': 59,\n",
       " 'fatal': 1,\n",
       " 'attack': 4,\n",
       " 'weaker': 2,\n",
       " 'mate': 1,\n",
       " 'human': 65,\n",
       " 'speci': 7,\n",
       " 'cultur': 23,\n",
       " 'darwinian': 1,\n",
       " 'strict': 4,\n",
       " 'canal': 1,\n",
       " 'man': 33,\n",
       " 'innum': 3,\n",
       " 'adapt': 22,\n",
       " 'ritual': 2,\n",
       " 'behavior': 52,\n",
       " 'analog': 11,\n",
       " 'homolog': 1,\n",
       " 'event': 12,\n",
       " 'fascin': 2,\n",
       " 'master': 18,\n",
       " 'manpow': 13,\n",
       " 'asheim': 2,\n",
       " 'offici': 9,\n",
       " 'statement': 33,\n",
       " 'offic': 34,\n",
       " 'explor': 45,\n",
       " 'eventu': 14,\n",
       " 'assumpt': 24,\n",
       " 'occup': 4,\n",
       " 'broader': 11,\n",
       " 'supervi': 4,\n",
       " 'requir': 151,\n",
       " 'complet': 78,\n",
       " 'encompass': 5,\n",
       " 'engag': 19,\n",
       " 'mainten': 17,\n",
       " 'norm': 7,\n",
       " 'defin': 97,\n",
       " 'pre': 13,\n",
       " 'posit': 47,\n",
       " 'pilot': 17,\n",
       " 'braden': 1,\n",
       " 'undertaken': 26,\n",
       " 'high': 54,\n",
       " 'uncertainti': 7,\n",
       " 'among': 99,\n",
       " 'regard': 48,\n",
       " 'extent': 29,\n",
       " 'miss': 12,\n",
       " 'gave': 8,\n",
       " 'ohio': 9,\n",
       " 'concret': 4,\n",
       " 'indic': 119,\n",
       " 'loss': 15,\n",
       " 'greatest': 19,\n",
       " 'convey': 9,\n",
       " 'voic': 5,\n",
       " 'complaint': 1,\n",
       " 'tend': 24,\n",
       " 'forc': 34,\n",
       " 'cuadra': 9,\n",
       " 'think': 28,\n",
       " 'fores': 2,\n",
       " 'move': 11,\n",
       " 'anyth': 4,\n",
       " 'tomorrow': 2,\n",
       " 'algebra': 6,\n",
       " 'maltsev': 1,\n",
       " 'far': 52,\n",
       " 'back': 18,\n",
       " 'twenti': 82,\n",
       " 'overwhelm': 7,\n",
       " 'algebraist': 1,\n",
       " 'investig': 120,\n",
       " 'ring': 12,\n",
       " 'lattic': 1,\n",
       " 'theoret': 64,\n",
       " 'arbitrari': 7,\n",
       " 'due': 18,\n",
       " 'birkhoff': 1,\n",
       " 'thirti': 66,\n",
       " 'five': 145,\n",
       " 'tarski': 1,\n",
       " 'formul': 43,\n",
       " 'equip': 33,\n",
       " 'contrast': 18,\n",
       " 'abund': 4,\n",
       " 'apparatu': 3,\n",
       " 'logic': 50,\n",
       " 'fruit': 8,\n",
       " 'classic': 7,\n",
       " 'discov': 23,\n",
       " 'next': 15,\n",
       " 'gradual': 11,\n",
       " 'becam': 20,\n",
       " 'intim': 4,\n",
       " 'despit': 10,\n",
       " 'speak': 12,\n",
       " 'singl': 57,\n",
       " 'formal': 57,\n",
       " 'predic': 4,\n",
       " 'calculu': 2,\n",
       " 'border': 3,\n",
       " 'doyl': 4,\n",
       " 'exploit': 16,\n",
       " 'word': 99,\n",
       " 'occurr': 13,\n",
       " 'retriev': 296,\n",
       " 'mental': 7,\n",
       " 'sever': 102,\n",
       " 'measur': 149,\n",
       " 'scrutin': 1,\n",
       " 'shown': 62,\n",
       " 'strongli': 7,\n",
       " 'occur': 41,\n",
       " 'pair': 10,\n",
       " 'map': 11,\n",
       " 'half': 38,\n",
       " 'mode': 22,\n",
       " 'search': 214,\n",
       " 'bourn': 6,\n",
       " 'estim': 48,\n",
       " ...}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6750"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "total_vocab_size = len(DF)\n",
    "total_vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6750"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_vocab = [x for x in DF]\n",
    "N=len(total_vocab)\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_freq(word):\n",
    "    c = 0\n",
    "    try:\n",
    "        c = DF[word]\n",
    "    except:\n",
    "        pass\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['languag', 'thought', 'poluskin', 'book', 'consid', 'basic', 'aspect', 'complex', 'problem', 'histor', 'social', 'essenc', 'languag', 'thought', 'interact', 'histor', 'evolut', 'essenc', 'linguist', 'mean', 'relat', 'content', 'side', 'thought', 'physiolog', 'mechan', 'process', 'abstract', 'gener', 'etc']\n",
      "tf-idf done\n"
     ]
    }
   ],
   "source": [
    "doc = 0\n",
    "N=len(tokens_set)\n",
    "tf_idf = {}\n",
    "\n",
    "for i in range(len(tokens_set)):\n",
    "    if(i>0):\n",
    "        tokens = tokens_set[str(i)]\n",
    "    if(i==0):\n",
    "        print(tokens)\n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = np.log((N+1)/(df+1))\n",
    "        \n",
    "        tf_idf[doc,token] = tf*idf\n",
    "    doc += 1\n",
    "\n",
    "print(\"tf-idf done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['languag',\n",
       " 'thought',\n",
       " 'poluskin',\n",
       " 'book',\n",
       " 'consid',\n",
       " 'basic',\n",
       " 'aspect',\n",
       " 'complex',\n",
       " 'problem',\n",
       " 'histor',\n",
       " 'social',\n",
       " 'essenc',\n",
       " 'languag',\n",
       " 'thought',\n",
       " 'interact',\n",
       " 'histor',\n",
       " 'evolut',\n",
       " 'essenc',\n",
       " 'linguist',\n",
       " 'mean',\n",
       " 'relat',\n",
       " 'content',\n",
       " 'side',\n",
       " 'thought',\n",
       " 'physiolog',\n",
       " 'mechan',\n",
       " 'process',\n",
       " 'abstract',\n",
       " 'gener',\n",
       " 'etc']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_set[\"1459\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cosine similarilty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(a, b):\n",
    "    cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.zeros((N, total_vocab_size))   #total_vocab_size is the length of DF\n",
    "for i in tf_idf:\n",
    "    try:\n",
    "        ind = total_vocab.index(i[1])\n",
    "        D[i[0]][ind] = tf_idf[i]\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_vector(tokens):\n",
    "\n",
    "    Q = np.zeros((len(total_vocab)))\n",
    "    \n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "\n",
    "    query_weights = {}\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = math.log((N+1)/(df+1))\n",
    "\n",
    "        try:\n",
    "            ind = total_vocab.index(token)\n",
    "            Q[ind] = tf*idf\n",
    "        except:\n",
    "            pass\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(k, query):\n",
    "    \n",
    "    preprocessed_query = preprocess(query)\n",
    "    tokens = word_tokenize(str(preprocessed_query))\n",
    "    \n",
    "    #print(\"\\nQuery:\", query)\n",
    "    \n",
    "    d_cosines = []\n",
    "    \n",
    "    query_vector = gen_vector(tokens)\n",
    "    \n",
    "    for d in D:\n",
    "        d_cosines.append(cosine_sim(query_vector, d))\n",
    "        \n",
    "    out = np.array(d_cosines).argsort()[-k:][::-1]\n",
    "    \n",
    "    \n",
    "    #print(\"Most similar Dpocuments-IDs : \")\n",
    "    \n",
    "    #print(out)\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Related documents to given query :\n",
      " \" What is information science?  Give definitions where possible. \" \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 469,  445, 1181, 1179,  540], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = cosine_similarity(5,qry_set[\"3\"])\n",
    "\n",
    "print('Related documents to given query :\\n \\\"', qry_set[\"3\"] , '\" \\n' )\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading ground truth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60, 85, 114, 123, 126, 131, 133, 136, 138, 140, 346, 359, 363, 372, 412, 445, 454, 461, 463, 469, 532, 537, 540, 553, 554, 555, 585, 590, 599, 640, 660, 664, 803, 901, 909, 911, 1027, 1053, 1169, 1179, 1181, 1190, 1191, 1326]\n"
     ]
    }
   ],
   "source": [
    "rel_set = {}\n",
    "with open('CISI/CISI.REL') as f:\n",
    "    for l in f.readlines():\n",
    "        qry_id = l.lstrip(\" \").strip(\"\\n\").split(\"\\t\")[0].split(\" \")[0]\n",
    "        doc_id = int(l.lstrip(\" \").strip(\"\\n\").split(\"\\t\")[0].split(\" \")[-1])\n",
    "        if qry_id in rel_set:\n",
    "            rel_set[qry_id].append(doc_id)\n",
    "        else:\n",
    "            rel_set[qry_id] = []\n",
    "            rel_set[qry_id].append(doc_id) \n",
    "    \n",
    "    \n",
    "print(rel_set[\"3\"]) # note that the dictionary indexes are strings, not numbers. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and recall, Accuracy and Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_list=[]\n",
    "recall_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_list=[]\n",
    "recall_list=[]\n",
    "accuracy_list=[]\n",
    "\n",
    "for i in range(1,len(doc_set)):\n",
    "    try:\n",
    "        result_from_cosine=cosine_similarity(6 , qry_set[str(i)]).tolist()\n",
    "        result_from_ground_truth=rel_set[str(i)]\n",
    "        \n",
    "        true_Positive=len(set(result_from_cosine) & set(result_from_ground_truth)) #set(a) & set(b) gives us intersection between a and b\n",
    "        false_Positive=len(np.setdiff1d(result_from_cosine , result_from_ground_truth))\n",
    "        false_Negative=len(np.setdiff1d(result_from_ground_truth , result_from_cosine))\n",
    "        true_negative= ( len(doc_set) -  (true_Positive + false_Negative + false_Positive) )\n",
    "        #print(\"true psotive\",true_Positive)\n",
    "        #print(\"false negative\",false_Negative)\n",
    "        \n",
    "        try:\n",
    "            precission= (true_Positive) / ( true_Positive + false_Positive )\n",
    "            recall= (true_Positive) / (true_Positive + false_Negative)\n",
    "            \n",
    "            accuracy= ( true_negative + true_Positive ) / (  true_negative + true_Positive + false_Negative +false_Positive)\n",
    "           \n",
    "        except ZeroDivisionError:\n",
    "            pass\n",
    "\n",
    "        precision_list.append(precission)\n",
    "        recall_list.append(recall)\n",
    "        accuracy_list.append(accuracy)\n",
    "        \n",
    "        \n",
    "        \n",
    "    except KeyError:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision=sum(precision_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "average_recall=sum(recall_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy= sum(accuracy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_Measure = (2 * average_precision * average_recall) / (average_precision + average_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision is :  32.333333333333336\n",
      "Average Recall is :  9.109436593357639\n",
      "F-score is :  14.214226045841656\n",
      "Accuracy :  73.82054794520543\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Precision is : \", average_precision)\n",
    "print(\"Average Recall is : \", average_recall)\n",
    "print(\"F-score is : \" ,F_Measure)\n",
    "print(\"Accuracy : \" ,Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter a Query, Get your Result \n",
    "### Simple User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Entered Query is :  test\n",
      "\n",
      "\n",
      "Related Documents IDs are :  [ 207 1154  134  595  827  149  223  817  851  423]\n",
      "\n",
      "Do you want to retrive the document ? \n",
      " press Y to see all related docs \n",
      " Press S to see a single document with given id \n",
      " Press N to exit \n",
      "\n",
      "\n",
      "*** You are in All Document Retriveal Mood ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Doc-Id : 207 \n",
      "\t Development of Methodologic Tools for Planning and Managing Library Services: II. Measuring a Library's Capability for Providing Documents Orr, Richard H. Pings, Vern M. Pizer, Irwin H. Olson, Edwin E. Spencer, Carol C. A method of measuring a library's capability for providing the documents its users need has been developed.. The library is tested with representative sample of such documents to determine how long would be required for users to obtain these documents.. Test results are expressed in terms of a Capability Index, which has a maximal value of 100 only if all the sample documents are found \\\"on shelf\\\".. Specific tests employing samples of 300 documents have been developed that are appropriate for academic an for \\\"reservoir\\\" biomedical libraries.. Realistic field trials have demonstrated that these two tests are practical to administer and that test results are adequately reproducible.. When strict comparability is not important, a library can test itself.. In assessing a reservoir library, test results are supplemented by data on its typical processing time for interlibrary loan requests.. Currently these tests are being used in a national survey.. The general method is applicable to other types of libraries, provided appropriate test samples are established.. If their limitations are clearly understood, these \\\"Document Delivery Tests\\\" can be valuable tools for planning and managing library services.. \n",
      "\n",
      "\n",
      "Doc-Id : 1154 \n",
      "\t Nonparametric Statistics for the Behavioral Sciences Siegel, S. In this book I have presented the tests according to the research design for which each is suited.  In discussing each test, I have attempted to indicate its \\\"function,\\\" i.e., to indicate the sort of data to which it is applicable, to convey some notion of the rationale of proof underlying the test, to explain its computation, to give examples of its application in behavioral scientific research, and to compare the test to its parametric equivalent, if any, and to any nonparametric tests of similar functions. \n",
      "\n",
      "\n",
      "Doc-Id : 134 \n",
      "\t Evaluation of Information Systems and Services Rees, A.M. This chapter summarizes and discusses the present state of the art in testing and evaluation.  Three tasks will be undertaken: to outline in some detail the few substantive research projects involving testing and evaluation, to describe a number of research projects in areas cognate to testing and evaluation, and finally, to provide some general conclusions with respect to past and future activity.  Although a distinction is made in this review between laboratory-based experimentation and tests of operational systems, the methodology used in each instance is substantially the same.  As yet, no full-scale and elaborate field approach has been attempted. \n",
      "\n",
      "\n",
      "Doc-Id : 595 \n",
      "\t Selected Results From An Inquiry Into Testing of Information Retrieval Systems Saracevic, Tefko A variety of aspects related to testing of retrieval systems were examined.. A model of a retrieval system, together with a set of measures and a methodology for performance testing were developed.. In the main experiment the effect on performance of the following variables was tested:  sources of indexing, indexing languages, coding schemes, question analyses, search strategies and formats of output.. In addition, a series of separate experiments was carried out to investigate the problems of controls in experimentation with IR systems.. The main conclusions:  the human factor appears to be the main variable in all components of an IR system; length of indexes affects performance considerably more than indexing languages; question analyses and search strategies to affect performance to a great extent - as much, if not  more than indexing.. Retrieval systems seem to be able to perform at present only on a general level, failing to be at the same time comprehensive and specific.. It seems that testing of total IR systems controlling and monitoring all factors (environmental and systems-related) is not possible at present.. \n",
      "\n",
      "\n",
      "Doc-Id : 827 \n",
      "\t The Evaluation of Information Retrieval Systems Farradane, J. Methods of testing systems in practice and in theory are critically reviewed.  Some new theoretical considerations are advanced. \n",
      "\n",
      "\n",
      "Doc-Id : 149 \n",
      "\t The Cranfield Tests on Index Language Devices Cleverdon, Cyril The investigation dealt with the effect which different devices have on the performance of index languages.. It appeared that the most important consideration was the specificity of the index terms; within the context of the conditions existing in this test, single-word terms were more effective than concept terms or a controlled vocabulary.. \n",
      "\n",
      "\n",
      "Doc-Id : 223 \n",
      "\t Document Delivery Capabilities of Major Biomedical Libraries in 1968: Results of a National Survey Employing Standardized Tests Orr, Richard H. Schless, Arthur P. The standardized Document Delivery Tests (DDT's) developed earlier (Bulletin 56: 241-267, July 1968) were employed to assess the capability of ninety-two medical school libraries for meeting the document needs of biomedical researchers, and the capability of fifteen major resource libraries for filling I-L requests from biomedical libraries.. The primary test data are summarized as statistics on the observed availability status of the 300 plus documents in the test samples, and as measures expressing capability as a function of the mean time that would be required for users to obtain test sample documents.. A mathematical model is developed in which the virtual capability of a library, as seen by its users, equals the algebraic sum of the basic capability afforded by its holdings; the combined losses attributable to use of its collection, processing, relative inaccessibility, and housekeeping problems; and the gain realized by coupling with other resources (I-L borrowing).. For a particular library, or group of libraries, empirical values for each of these variables can be  calculated easily from the capability measures and the status statistics.. Regression equations are derived that provide useful predictions of basic capability from collection size.. The most important result of this work is that cost-effectiveness analyses can now be used as practical decision aids in managing a basic library service.. A program of periodic surveys and further development of DDT's is recommended as appropriate for the Medical Library Association.. \n",
      "\n",
      "\n",
      "Doc-Id : 817 \n",
      "\t The Aberrystwyth Index Languages Test Keen, Michael E. Reports a laboratory comparision of the effectiveness and efficiency of five index languages in the subject area of library and information science; three post-co-ordinate languages, Compressed Term, Uncontrolled, and Hierarchically Structured, and two pre-co-ordinate ones, Hierarchically Structures and Relational Indexing.. Eight test comparisons were made, and factors studied were index language specificity and linkage, indexing specificity and exhaustivity, method of co-ordination, the precision devices of partitioning and relational operators, and the provision of context in the search file.. Full details of the test and retrieval results are presented.. \n",
      "\n",
      "\n",
      "Doc-Id : 851 \n",
      "\t Bibliographic Retrieval from Bibliographic Input; The Hypothesis and Construction of a Test Ruecking, Frederick H. Jr. A study of problems associated with bibliographic retrieval using unverified input data supplied by requesters.. A code derived from compression of title and author information to four, four-character abbreviations each was used for retrieval tests on an IBM 1401 computer.. Retrieval accuracy was 98.67%.. \n",
      "\n",
      "\n",
      "Doc-Id : 423 \n",
      "\t R and D Project Selection:  Where We Stand Baker, N.R. Pound, W.H. A review of the literature on R and D project selection and an analysis of interview data suggest that there is a lack of testing and use of the methods proposed.. Several OR-MS methods are identified and their current status is indicated.. Three representative procedures are examined in some detail.. It is argued that both a lack of testing concerning feasibility and shortcomings of the models themselves, help to explain why the methods have not been used.. Some of these shortcomings are identified and discussed.. Implications for future research are presented.. \n"
     ]
    }
   ],
   "source": [
    "query=input(\"Enter your query here : \")\n",
    "\n",
    "Q=cosine_similarity(10,query)\n",
    "\n",
    "print(\"\\n\\nEntered Query is : \" , query)\n",
    "print(\"\\n\\nRelated Documents IDs are : \", Q)\n",
    "print(\"\\nDo you want to retrive the document ? \\n press Y to see all related docs \\n Press S to see a single document with given id \\n Press N to exit \")\n",
    "\n",
    "entered_option=input()\n",
    "    \n",
    "if entered_option == \"Y\":\n",
    "\n",
    "    print(\"\\n\\n*** You are in All Document Retriveal Mood ***\\n\\n\")\n",
    "\n",
    "    for i in range(len(Q)):\n",
    "            print(\"\\n\\nDoc-Id :\", Q[i] , \"\\n\\t\" ,doc_set[str(Q[i])])\n",
    "           \n",
    "elif entered_option == \"S\":\n",
    "    print(\"Enter your desired document ID : \")\n",
    "    doc_id=input()\n",
    "    print(\"Doc-Id : \", doc_id, \"\\n\\t\" ,doc_set[doc_id])\n",
    "        \n",
    "\n",
    "else:\n",
    "    print(\"Thank you for using our Information System\")\n",
    "    print(\"Hassan Ashiq & Usman Ali Abbasi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On Some Clustering Techniques Bonner, R.E. The problem of organizing a large mass of data occurs frequently in research.. Normally, some process of generalization is used to compress the data so that it can be analyzed more easily.. A primitive step in this process is the \\\\\"clustering\\\\\" technique, which involves gathering together similar data into a cluster to permit a significant generalization.. This paper describes a number of methods which make use of IBM 7090 computer programs to do clustering.. A medical research problem is used to illustrate and compare these methods.. '"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_set[\"422\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## IR Semester Project\n",
    " ### NUST\n",
    "- #### Submitted by : Hassan Ashiq BESE 23 C \n",
    "- ###### Link to my GitHub Repository <a href=\"https://github.com/hassanashiqasse/PCA\">Click Here</a>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
